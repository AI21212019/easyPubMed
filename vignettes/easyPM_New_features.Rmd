---
title: "easyPubMed ver:2.9 - New features"
author: "Damiano Fantini"
date: "September 17, 2018"
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
vignette: |
  %\VignetteIndexEntry{Retrieving and Analyzing PubMed Records via easyPubMed} %\VignetteEngine{knitr::rmarkdown} %\usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("/home/dami/Documents/r_packs_dev/easyPubmed/ver_2_6/down/")
library(easyPubMed)
library(parallel)
library(foreach)
library(doParallel)
```

## Install new version of easyPubMed from GitHub

The following code is aimed at downloading an installing the *dev* version of `easyPubMed` from GitHub. You'll need to install the `devtools` library first (available on CRAN).
```{r example_001, include=TRUE, eval=FALSE}
# Install easyPubMed v2.7
library(devtools)
install_github("dami82/easyPubMed")

# Load library
library(easyPubMed)
```

## Getting started - prep some data

The following code is aimed at downloading ad pre-processing a list of ~6000 PubMed records. These records will be downloaded in a series of 1000-record batches.
```{r example_002}
# Query pubmed and fetch many results
my_query <- 'Interleukin[TI] AND "2015"[PDAT]:"2017"[PDAT]'
my_query <- get_pubmed_ids(my_query)

# Download by 1000-item batches
my_batches <- seq(from = 1, to = my_query$Count, by = 1000)
my_abstracts_xml <- lapply(my_batches,  function(i) {
  fetch_pubmed_data(my_query, retmax = 1000, retstart = i)  
})

# Store Pubmed Records as elements of a list
all_xml <- list()
for(x in my_abstracts_xml) {
  xx <- articles_to_list(x)
  for(y in xx) {
    all_xml[[(1 + length(all_xml))]] <- y
  }  
}
```

## Demo 1: fast extraction of PMID, Title, and Abstract

The following code illustrates the use of `article_to_df(, getAuthors = FALSE)`, for fast extraction of PubMed record titles and abstracts. This function can process PubMed records quickly, and will return all info except author info. Here, ~6000 records were processed in less than 2 min.

```{r example_003}
# Starting time: record
t.start <- Sys.time()

# Perform operation (use lapply here, no further parameters)
final_df <- do.call(rbind, lapply(all_xml, article_to_df, 
                                  max_chars = -1, getAuthors = FALSE))

# Final time: record
t.stop <- Sys.time()

# How long did it take?
print(t.stop - t.start)
```
```{r example_003bis, include=FALSE, echo=FALSE}
# Show an excerpt of the results
final_df$abstract <- substr(final_df$abstract, 1, 25)
final_df$abstract <- paste(final_df$abstract, "...", sep = "")
```
```{r example_003tris}
# Show an excerpt of the results
head(final_df[,c("pmid", "year", "abstract")])

# If interested in specific information,
# you can subset the dataframe and save the
# desired columns/features
id_abst_df <- final_df[,c("pmid", "abstract")]
head(id_abst_df)
```

## Demo 2: full info extraction, including keywords

The following code illustrates the use of `article_to_df(, getKeywords = TRUE)`, for recursive extraction of PubMed record info, including keywords. Author info extraction is a time-consuming process. Here, we are extracting info from the first 1000 PubMed records returned by the query. Here, 1000 records were processed in ~3 min.

```{r example_004}
# Starting time: record
t.start <- Sys.time()

# Perform operation (use lapply here, no further parameters)
keyword_df <- do.call(rbind, lapply(all_xml[1:1000], 
                                    article_to_df, autofill = T, 
                                    max_chars = 100, getKeywords = T))

# Final time: record
t.stop <- Sys.time()

# How long did it take?
print(t.stop - t.start)
```
```{r example_004bis, include=FALSE, echo=FALSE}
# Show an excerpt of the results
keyword_df$keywords <- ifelse(is.na(keyword_df$keywords), 
                              NA, paste(substr(keyword_df$keywords, 1, 100), "...", sep = ""))
```
```{r example_004tris}
# Visualize Keywords extracted from PubMed records
# Keyword and MeSH Concepts are separated by semicolons
print(keyword_df$keywords[1])
```
```{r example_004quatr, include=FALSE, echo=FALSE}
# Show an excerpt of the results
keyword_df$keywords <- ifelse(is.na(keyword_df$keywords), 
                              NA, paste(substr(keyword_df$keywords, 1, 30), "...", sep = ""))
```
```{r example_004penta}
# Show an excerpt of the results
keyword_df[seq(1, 200, by = 10), c("lastname", "firstname", "keywords")]
```

## Demo 3: full info extraction via parallelization

The following code illustrates the use of `article_to_df()` in conjunction with parallelization. If multiple cores are available, splitting the job in multiple tasks can support faster info extraction from a large number of records. Here, ~6000 records were processed in ~11 min.

```{r example_005}
# Load required packages (available from CRAN).
# This will work on UNIX/LINUX systems. 
# Windows systems may not support the following code.
library(parallel)
library(foreach)
library(doParallel)

# Starting time: record
t.start <- Sys.time()

# Start a cluster with 5 cores
cl <- makeCluster(5)
registerDoParallel(cl)

# Perform operation (use foreach)
# The .combine argument guides result aggregation
fullDF <- tryCatch(
  {foreach(x=all_xml, 
           .packages = 'easyPubMed',
           .combine = rbind) %dopar% article_to_df(pubmedArticle = x, 
                                                   autofill = T, 
                                                   max_chars = 500, 
                                                   getKeywords = T, 
                                                   getAuthors = T)}, 
  error = function(e) {NULL},
  finally = {stopCluster(cl)})

# Final time: record
t.stop <- Sys.time()

# How long did it take?
print(t.stop - t.start)
```
```{r example_005bis, include=FALSE, echo=FALSE}
# Show an excerpt of the results
fullDF$keywords <- ifelse(is.na(fullDF$keywords), 
                          NA, paste(substr(fullDF$keywords, 1, 15), "...", sep = ""))
fullDF$abstract <- ifelse(is.na(fullDF$abstract), 
                          NA, paste(substr(fullDF$abstract, 1, 20), "...", sep = ""))
```
```{r example_005tris}
# Show an excerpt of the results
fullDF[seq(1, 200, by = 10), c("lastname", "keywords", "abstract")]
```

## Demo 4: Faster queries using API key.

The following code illustrates the use of the argument `api_key`, which was introduced in version 2.9. E-utils users are allowed 3 requests/second without an API key. However, users can obtain an API key to increase the e-utils limit to 10 requests/second. For more information, visit: https://www.ncbi.nlm.nih.gov/account/settings/. Three `easyPubMed` functions can take the `api_key` argument: `get_pubmed_ids()`, `fetch_pubmed_data()`, and `batch_pubmed_download()`. Requests submitted by the latter function are automatically paced, therefore the use of a key may speed the queries if records are retrieved in small batches.

```{r example_006_show, eval=FALSE}
# define a PubMed Query: this should return 40 results
my_query <- '"immune checkpoint" AND 2010[DP]:2012[DP]'

# Monitor time, and proceed with record download -- USING API_KEY!
t_key1 <- Sys.time()
batch_pubmed_download(my_query, 
                      api_key = "4ea263f0f8e9108aee96ace507afXXXXXXXX", 
                      batch_size = 2, dest_file_prefix = "TMP_api_")
t_key2 <- Sys.time()

# Monitor time, and proceed with record download -- DO NOT USE API_KEY!
t_nok1 <- Sys.time()
batch_pubmed_download(my_query, 
                      batch_size = 2, dest_file_prefix = "TMP_no_")
t_nok2 <- Sys.time()
```

```{r example_006_run, eval=TRUE, echo=FALSE, include=FALSE}
# Show an excerpt of the results
setwd("/home/dami/Documents/TMPP/")
my_query <- '"immune checkpoint" AND 2010[DP]:2012[DP]'
t_key1 <- Sys.time()
batch_pubmed_download(my_query, 
                      api_key = "4ea263f0f8e9108aee96ace507af23a4eb09", 
                      batch_size = 2, dest_file_prefix = "TMP_api_")
t_key2 <- Sys.time()

t_nok1 <- Sys.time()
batch_pubmed_download(my_query, 
                      batch_size = 2, dest_file_prefix = "TMP_no_")
t_nok2 <- Sys.time()

```

```{r example_006_close, eval=TRUE}
# Compute time differences
# The use of a key makes the process faster
print(paste("With key:", t_key2 - t_key1))
print(paste("W/o key:", t_nok2 - t_nok1))
```

## Acknowledgements
Thank you for using `easyPubMed`. This software was developed by Damiano Fantini. This vignette was created and built in September 2018. Please, acknowledge Damiano Fantini's work and http://www.data-pulse.com when using this code and/or `easyPubMed`. Thank you. (C-2018 - Damiano Fantini). More info available at http://www.data-pulse.com

## Session Info

```{r sein_005}
# Session Info
print(sessionInfo())
```


